{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import pandas as pd\n",
    "import io\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "import textdata\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from pays import Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pays\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f3c1807e748>: Failed to establish a new connection: [Errno -3] Échec temporaire dans la résolution du nom')': /simple/pays/\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/e1/aad71394624467fbe91cc88f257cadb2dee758b3ba1e11cf88ff1044e1ee/pays-0.2.3.tar.gz (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pays\n",
      "  Running setup.py bdist_wheel for pays ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/cmisid/.cache/pip/wheels/00/6b/52/3741cf41083453c269d6948d6716d84363e464fb5698ee5c6f\n",
      "Successfully built pays\n",
      "Installing collected packages: pays\n",
      "Successfully installed pays-0.2.3\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "#!pip install pays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Allow us to tokenize the texte pased in parameter \n",
    "        with the help of word_tokenize method taken from nltk library\n",
    "        @param text : the text to tokenize\n",
    "        @return : a tokenized text\n",
    "    \"\"\"\n",
    "    return ' '.join(word_tokenize(unidecode(text)))\n",
    "\n",
    "def tf(text):\n",
    "    \"\"\"\n",
    "        Summary:\n",
    "            this functions gives a dictionnaire where we have the TF associated\n",
    "            to each work in an article\n",
    "        In:\n",
    "            - text already tokenize\n",
    "        Out:\n",
    "            - TF for each word of the article gave in a dictionary\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(text, return_counts=True)\n",
    "    new_count = map(np.asscalar, counts)\n",
    "    dict_words = dict(zip(unique, new_count))\n",
    "    return dict_words\n",
    "\n",
    "def is_country(word)\n",
    "    list_countries = [str(country).lower() for country in Countries('fra')]\n",
    "    return 1 if word in list_coutries else 0\n",
    "\n",
    "def clean_text(articles):\n",
    "\n",
    "    stop_words = [\" \", \"  \", \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\",\n",
    "                  \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\",\n",
    "                  \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\",\n",
    "                  \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\",\n",
    "                  \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autre\", \"autrefois\", \"autrement\",\n",
    "                  \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\",\n",
    "                  \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\", \"bah\", \"bas\", \"basee\",\n",
    "                  \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"brrr\", \"c\", \"car\", \"ce\", \"ceci\",\n",
    "                  \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\",\n",
    "                  \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\",\n",
    "                  \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\",\n",
    "                  \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\",\n",
    "                  \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"com\", \"comme\", \"comment\", \"comparable\",\n",
    "                  \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"d\", \"da\", \"dans\", \"de\", \"debout\",\n",
    "                  \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\",\n",
    "                  \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\",\n",
    "                  \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\",\n",
    "                  \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\",\n",
    "                  \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\",\n",
    "                  \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\",\n",
    "                  \"dès\", \"début\", \"désormais\", \"e\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\",\n",
    "                  \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\",\n",
    "                  \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\",\n",
    "                  \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\",\n",
    "                  \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\",\n",
    "                  \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\",\n",
    "                  \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"g\", \"gens\", \"h\", \"ha\", \"haut\",\n",
    "                  \"hein\", \"hem\", \"hep\", \"hi\", \"ho\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\",\n",
    "                  \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\",\n",
    "                  \"jusqu\", \"jusque\", \"juste\", \"k\", \"l\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\",\n",
    "                  \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\",\n",
    "                  \"lui-même\", \"là\", \"lès\", \"m\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\",\n",
    "                  \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\",\n",
    "                  \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\",\n",
    "                  \"multiples\", \"même\", \"mêmes\", \"n\", \"na\", \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\",\n",
    "                  \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\",\n",
    "                  \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\",\n",
    "                  \"nôtres\", \"o\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\",\n",
    "                  \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"o|\", \"où\", \"p\", \"paf\", \"pan\",\n",
    "                  \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\",\n",
    "                  \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\",\n",
    "                  \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\",\n",
    "                  \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\",\n",
    "                  \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\",\n",
    "                  \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\",\n",
    "                  \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"q\", \"qu\", \"quand\", \"quant\",\n",
    "                  \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\",\n",
    "                  \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\",\n",
    "                  \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\",\n",
    "                  \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\",\n",
    "                  \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"s\", \"sa\", \"sacrebleu\", \"sait\", \"sans\",\n",
    "                  \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\",\n",
    "                  \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\",\n",
    "                  \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\",\n",
    "                  \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\",\n",
    "                  \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\",\n",
    "                  \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\",\n",
    "                  \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\",\n",
    "                  \"surtout\", \"t\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\",\n",
    "                  \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\",\n",
    "                  \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\",\n",
    "                  \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\",\n",
    "                  \"tu\", \"té\", \"u\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"v\", \"va\", \"vais\",\n",
    "                  \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\",\n",
    "                  \"voie\", \"voient\", \"voilà\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\",\n",
    "                  \"vôtres\", \"w\", \"x\", \"y\", \"z\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\",\n",
    "                  \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\",'.','...','?','!',':']\n",
    "    det = [\"le\", \"la\", \"les\", \"l'\", \"un\", \"une\", \"des\", \"d'\", \"du\", \"de\", \"au\", \"aux\", \"ce\", \"cet\", \"cette\", \"ces\",\n",
    "           \"mon\", \"son\", \"ma\", \"ta\", \"sa\", \"mes\", \"ses\", \"notre\", \"votre\", \"leur\", \"nos\", \"vos\", \"leurs\", \"aucun\",\n",
    "           \"aucune\", \"aucuns\", \"tel\", \"telle\", \"tels\", \"telles\", \"tout\", \"toute\", \"tous\", \"toutes\", \"chaque\"]\n",
    "    pron = [\"je\", \"tu\", \"il\", \"elle\", \"on\", \"nous\", \"vous\", \"ils\", \"elles\", \"me\", \"m'\", \"moi\", \"te\", \"t'\", \"toi\",\n",
    "              \"se\", \"y\", \"le\", \"lui\", \"soi\", \"leur\", \"eux\", \"lui\", \"qui\", \"que\", \"quoi\", \"dont\", \"où\"]\n",
    "    conj = [\"mais\", \"ou\", \"et\", \"donc\", \"or\", \"ni\", \"car\", \"que\", \"quand\", \"comme\", \"si\", \"lorsque\", \"quoique\",\n",
    "                   \"puisque\"]\n",
    "    prep = [\"à\", \"derrière\", \"malgré\", \"sauf\", \"selon\", \"avant\", \"devant\", \"sous\", \"avec\", \"en\", \"par\", \"sur\",\n",
    "                    \"entre\", \"parmi\", \"envers\", \"pendant\", \"vers\", \"dans\", \"pour\", \"de\", \"près\", \"depuis\", \"sans\"]\n",
    "\n",
    "    new_articles = {}\n",
    "    for i in tqdm_notebook(articles):\n",
    "        \n",
    "        new_articles[i] = articles[i]\n",
    "        \n",
    "        # Clean the text, get only relevant words\n",
    "        new_articles[i]['vocab'] = [word.lower() for word in textdata.words(new_articles[i]['content']+ new_articles[i]['content']) if len(word) > 2]\n",
    "        new_articles[i]['vocab'] = [word for word in new_articles[i]['vocab'] if word not in stop_words and word not in det and word not in pron and word not in conj and word not in prep]\n",
    "        new_articles[i]['vocab'] = [word for word in new_articles[i]['vocab'] if (\"http\" not in word) and (\"www\" not in word) and (\".\" not in word) and (\"(\" not in word)  and (\")\" not in word) and (\" \" not in word) and (\",\" not in word) and (len(word) < 16)]                                             \n",
    "        \n",
    "        # Get the tf for each word of the vocab for each article\n",
    "        new_articles[i]['rough_words'] = tf(new_articles[i]['vocab'])\n",
    "        \n",
    "        # Clean the words containing \"'\" caracter\n",
    "        new_articles[i]['words'] = {}\n",
    "        for word in new_articles[i]['rough_words'].keys():\n",
    "            if \"'\" in word and len(word.split(\"'\")[1]) > 3 and word.split(\"'\")[1] not in stop_words and word.split(\"'\")[1] not in det and word.split(\"'\")[1] not in pron and word.split(\"'\")[1] not in conj and word.split(\"'\")[1] not in prep :\n",
    "                new_articles[i]['words'][word.split(\"'\")[1]] = new_articles[i]['rough_words'][word]\n",
    "            else:\n",
    "                new_articles[i]['words'][word] = new_articles[i]['rough_words'][word]\n",
    "            \n",
    "        \n",
    "        # Get the tf for each word of the vocab for each article\n",
    "        new_articles[i]['rough_words'] = tf(new_articles[i]['vocab'])\n",
    "                \n",
    "        # Get the pos tag for each word\n",
    "        pos_tagging = nltk.pos_tag(new_articles[i]['vocab'])\n",
    "        for pos_tag,word in zip(pos_tagging,new_articles[i]['words'].keys()):\n",
    "            title = 0\n",
    "            description = 0\n",
    "            if 'description' in  new_articles[i].keys() and word in new_articles[i]['description']:\n",
    "                description = 1\n",
    "            if word in new_articles[i]['title']:\n",
    "                title = 1\n",
    "            new_articles[i]['words'][word] = {'tf':new_articles[i]['words'][word],\n",
    "                                             'is_title':title,\n",
    "                                             'is_description':description,\n",
    "                                             'is_country': is_country(word)\n",
    "                                             'pos_tag':pos_tag[1]}\n",
    "            \n",
    "        if 'description' in  new_articles[i].keys():\n",
    "            del new_articles[i]['description']\n",
    "        del new_articles[i]['rough_words']\n",
    "        del new_articles[i]['title']\n",
    "        del new_articles[i]['content']\n",
    "        del new_articles[i]['id_article']\n",
    "        del new_articles[i]['vocab']\n",
    "\n",
    "    return new_articles\n",
    "\n",
    "\n",
    "def create_json_file(articles,name,path):\n",
    "    \"\"\"\n",
    "        Transform the list containing info on articles into json file\n",
    "        @param articles : all cleaned articles \n",
    "        @param i : the clean article id\n",
    "        @param path : path where the json files are stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(articles)\n",
    "        filename = 'clean_art_' + name+'.json'\n",
    "        with open(path + filename, 'w', encoding='utf-8') as file:\n",
    "            df.to_json(file,force_ascii=False) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal de l'environnnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26630cecf3f46849ce3ba8b1e91f300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=284), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get scraped articles\n",
    "articles_jdle = json.loads(io.open('../data/articles/rough_art_jdle.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,description and title element for jdle articles\n",
    "for i in articles_jdle:\n",
    "    articles_jdle[str(i)]['content'] = tokenize(articles_jdle[str(i)]['content'])\n",
    "    articles_jdle[str(i)]['description'] = tokenize(articles_jdle[str(i)]['description'])\n",
    "    articles_jdle[str(i)]['title'] = tokenize(articles_jdle[str(i)]['title'])\n",
    "    \n",
    "# Cleaned articles\n",
    "clean_articles_jdle = clean_text(articles_jdle)\n",
    "create_json_file(clean_articles_jdle,'jdle','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16d2cff88a6450dba31ea4502d7724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=541), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get scraped articles\n",
    "articles_lmde = json.loads(io.open('../data/articles/rough_art_lmde.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,description and title element for lmde articles\n",
    "for i in articles_lmde:\n",
    "    articles_lmde[str(i)]['content'] = tokenize(articles_lmde[str(i)]['content'])\n",
    "    articles_lmde[str(i)]['description'] = tokenize(articles_lmde[str(i)]['description'])\n",
    "    articles_lmde[str(i)]['title'] = tokenize(articles_lmde[str(i)]['title'])\n",
    "    \n",
    "# Cleaned articles\n",
    "clean_articles_lmde = clean_text(articles_lmde)\n",
    "create_json_file(clean_articles_lmde,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7eme Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778652d0507f4dfaae3dd7b08ac768d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=135), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get scraped articles\n",
    "articles_7econ = json.loads(io.open('../data/articles/rough_art_7econ.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,descirotion and title element for 7econ articles\n",
    "for i in articles_7econ:\n",
    "    articles_7econ[str(i)]['content'] = tokenize(articles_7econ[str(i)]['content'])\n",
    "    articles_7econ[str(i)]['title'] = tokenize(articles_7econ[str(i)]['title'])\n",
    "\n",
    "# Cleaned articles\n",
    "clean_articles_7econ = clean_text(articles_7econ)\n",
    "create_json_file(clean_articles_7econ,'7econ','../data/articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
