{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import pandas as pd\n",
    "import io\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import json\n",
    "import textdata\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Allow us to tokenize the texte pased in parameter \n",
    "        with the help of word_tokenize method taken from nltk library\n",
    "        @param text : the text to tokenize\n",
    "        @return : a tokenized text\n",
    "    \"\"\"\n",
    "    return ' '.join(word_tokenize(unidecode(text)))\n",
    "\n",
    "def clean_text(articles):\n",
    "\n",
    "    stop_words = [\" \", \"  \", \"a\", \"abord\", \"absolument\", \"afin\", \"ah\", \"ai\", \"aie\", \"aient\", \"aies\", \"ailleurs\",\n",
    "                  \"ainsi\", \"ait\", \"allaient\", \"allo\", \"allons\", \"allô\", \"alors\", \"anterieur\", \"anterieure\",\n",
    "                  \"anterieures\", \"apres\", \"après\", \"as\", \"assez\", \"attendu\", \"au\", \"aucun\", \"aucune\", \"aucuns\",\n",
    "                  \"aujourd\", \"aujourd'hui\", \"aupres\", \"auquel\", \"aura\", \"aurai\", \"auraient\", \"aurais\", \"aurait\",\n",
    "                  \"auras\", \"aurez\", \"auriez\", \"aurions\", \"aurons\", \"auront\", \"aussi\", \"autre\", \"autrefois\", \"autrement\",\n",
    "                  \"autres\", \"autrui\", \"aux\", \"auxquelles\", \"auxquels\", \"avaient\", \"avais\", \"avait\", \"avant\", \"avec\",\n",
    "                  \"avez\", \"aviez\", \"avions\", \"avoir\", \"avons\", \"ayant\", \"ayez\", \"ayons\", \"b\", \"bah\", \"bas\", \"basee\",\n",
    "                  \"bat\", \"beau\", \"beaucoup\", \"bien\", \"bigre\", \"bon\", \"boum\", \"bravo\", \"brrr\", \"c\", \"car\", \"ce\", \"ceci\",\n",
    "                  \"cela\", \"celle\", \"celle-ci\", \"celle-là\", \"celles\", \"celles-ci\", \"celles-là\", \"celui\", \"celui-ci\",\n",
    "                  \"celui-là\", \"celà\", \"cent\", \"cependant\", \"certain\", \"certaine\", \"certaines\", \"certains\", \"certes\",\n",
    "                  \"ces\", \"cet\", \"cette\", \"ceux\", \"ceux-ci\", \"ceux-là\", \"chacun\", \"chacune\", \"chaque\", \"cher\", \"chers\",\n",
    "                  \"chez\", \"chiche\", \"chut\", \"chère\", \"chères\", \"ci\", \"cinq\", \"cinquantaine\", \"cinquante\",\n",
    "                  \"cinquantième\", \"cinquième\", \"clac\", \"clic\", \"combien\", \"com\", \"comme\", \"comment\", \"comparable\",\n",
    "                  \"comparables\", \"compris\", \"concernant\", \"contre\", \"couic\", \"crac\", \"d\", \"da\", \"dans\", \"de\", \"debout\",\n",
    "                  \"dedans\", \"dehors\", \"deja\", \"delà\", \"depuis\", \"dernier\", \"derniere\", \"derriere\", \"derrière\", \"des\",\n",
    "                  \"desormais\", \"desquelles\", \"desquels\", \"dessous\", \"dessus\", \"deux\", \"deuxième\", \"deuxièmement\",\n",
    "                  \"devant\", \"devers\", \"devra\", \"devrait\", \"different\", \"differentes\", \"differents\", \"différent\",\n",
    "                  \"différente\", \"différentes\", \"différents\", \"dire\", \"directe\", \"directement\", \"dit\", \"dite\", \"dits\",\n",
    "                  \"divers\", \"diverse\", \"diverses\", \"dix\", \"dix-huit\", \"dix-neuf\", \"dix-sept\", \"dixième\", \"doit\",\n",
    "                  \"doivent\", \"donc\", \"dont\", \"dos\", \"douze\", \"douzième\", \"dring\", \"droite\", \"du\", \"duquel\", \"durant\",\n",
    "                  \"dès\", \"début\", \"désormais\", \"e\", \"effet\", \"egale\", \"egalement\", \"egales\", \"eh\", \"elle\", \"elle-même\",\n",
    "                  \"elles\", \"elles-mêmes\", \"en\", \"encore\", \"enfin\", \"entre\", \"envers\", \"environ\", \"es\", \"essai\", \"est\",\n",
    "                  \"et\", \"etant\", \"etc\", \"etre\", \"eu\", \"eue\", \"eues\", \"euh\", \"eurent\", \"eus\", \"eusse\", \"eussent\",\n",
    "                  \"eusses\", \"eussiez\", \"eussions\", \"eut\", \"eux\", \"eux-mêmes\", \"exactement\", \"excepté\", \"extenso\",\n",
    "                  \"exterieur\", \"eûmes\", \"eût\", \"eûtes\", \"f\", \"fais\", \"faisaient\", \"faisant\", \"fait\", \"faites\", \"façon\",\n",
    "                  \"feront\", \"fi\", \"flac\", \"floc\", \"fois\", \"font\", \"force\", \"furent\", \"fus\", \"fusse\", \"fussent\",\n",
    "                  \"fusses\", \"fussiez\", \"fussions\", \"fut\", \"fûmes\", \"fût\", \"fûtes\", \"g\", \"gens\", \"h\", \"ha\", \"haut\",\n",
    "                  \"hein\", \"hem\", \"hep\", \"hi\", \"ho\", \"holà\", \"hop\", \"hormis\", \"hors\", \"hou\", \"houp\", \"hue\", \"hui\",\n",
    "                  \"huit\", \"huitième\", \"hum\", \"hurrah\", \"hé\", \"hélas\", \"i\", \"ici\", \"il\", \"ils\", \"importe\", \"j\", \"je\",\n",
    "                  \"jusqu\", \"jusque\", \"juste\", \"k\", \"l\", \"la\", \"laisser\", \"laquelle\", \"las\", \"le\", \"lequel\", \"les\",\n",
    "                  \"lesquelles\", \"lesquels\", \"leur\", \"leurs\", \"longtemps\", \"lors\", \"lorsque\", \"lui\", \"lui-meme\",\n",
    "                  \"lui-même\", \"là\", \"lès\", \"m\", \"ma\", \"maint\", \"maintenant\", \"mais\", \"malgre\", \"malgré\", \"maximale\",\n",
    "                  \"me\", \"meme\", \"memes\", \"merci\", \"mes\", \"mien\", \"mienne\", \"miennes\", \"miens\", \"mille\", \"mince\", \"mine\",\n",
    "                  \"minimale\", \"moi\", \"moi-meme\", \"moi-même\", \"moindres\", \"moins\", \"mon\", \"mot\", \"moyennant\", \"multiple\",\n",
    "                  \"multiples\", \"même\", \"mêmes\", \"n\", \"na\", \"naturel\", \"naturelle\", \"naturelles\", \"ne\", \"neanmoins\",\n",
    "                  \"necessaire\", \"necessairement\", \"neuf\", \"neuvième\", \"ni\", \"nombreuses\", \"nombreux\", \"nommés\", \"non\",\n",
    "                  \"nos\", \"notamment\", \"notre\", \"nous\", \"nous-mêmes\", \"nouveau\", \"nouveaux\", \"nul\", \"néanmoins\", \"nôtre\",\n",
    "                  \"nôtres\", \"o\", \"oh\", \"ohé\", \"ollé\", \"olé\", \"on\", \"ont\", \"onze\", \"onzième\", \"ore\", \"ou\", \"ouf\",\n",
    "                  \"ouias\", \"oust\", \"ouste\", \"outre\", \"ouvert\", \"ouverte\", \"ouverts\", \"o|\", \"où\", \"p\", \"paf\", \"pan\",\n",
    "                  \"par\", \"parce\", \"parfois\", \"parle\", \"parlent\", \"parler\", \"parmi\", \"parole\", \"parseme\", \"partant\",\n",
    "                  \"particulier\", \"particulière\", \"particulièrement\", \"pas\", \"passé\", \"pendant\", \"pense\", \"permet\",\n",
    "                  \"personne\", \"personnes\", \"peu\", \"peut\", \"peuvent\", \"peux\", \"pff\", \"pfft\", \"pfut\", \"pif\", \"pire\",\n",
    "                  \"pièce\", \"plein\", \"plouf\", \"plupart\", \"plus\", \"plusieurs\", \"plutôt\", \"possessif\", \"possessifs\",\n",
    "                  \"possible\", \"possibles\", \"pouah\", \"pour\", \"pourquoi\", \"pourrais\", \"pourrait\", \"pouvait\", \"prealable\",\n",
    "                  \"precisement\", \"premier\", \"première\", \"premièrement\", \"pres\", \"probable\", \"probante\", \"procedant\",\n",
    "                  \"proche\", \"près\", \"psitt\", \"pu\", \"puis\", \"puisque\", \"pur\", \"pure\", \"q\", \"qu\", \"quand\", \"quant\",\n",
    "                  \"quant-à-soi\", \"quanta\", \"quarante\", \"quatorze\", \"quatre\", \"quatre-vingt\", \"quatrième\",\n",
    "                  \"quatrièmement\", \"que\", \"quel\", \"quelconque\", \"quelle\", \"quelles\", \"quelqu'un\", \"quelque\", \"quelques\",\n",
    "                  \"quels\", \"qui\", \"quiconque\", \"quinze\", \"quoi\", \"quoique\", \"r\", \"rare\", \"rarement\", \"rares\",\n",
    "                  \"relative\", \"relativement\", \"remarquable\", \"rend\", \"rendre\", \"restant\", \"reste\", \"restent\",\n",
    "                  \"restrictif\", \"retour\", \"revoici\", \"revoilà\", \"rien\", \"s\", \"sa\", \"sacrebleu\", \"sait\", \"sans\",\n",
    "                  \"sapristi\", \"sauf\", \"se\", \"sein\", \"seize\", \"selon\", \"semblable\", \"semblaient\", \"semble\", \"semblent\",\n",
    "                  \"sent\", \"sept\", \"septième\", \"sera\", \"serai\", \"seraient\", \"serais\", \"serait\", \"seras\", \"serez\",\n",
    "                  \"seriez\", \"serions\", \"serons\", \"seront\", \"ses\", \"seul\", \"seule\", \"seulement\", \"si\", \"sien\", \"sienne\",\n",
    "                  \"siennes\", \"siens\", \"sinon\", \"six\", \"sixième\", \"soi\", \"soi-même\", \"soient\", \"sois\", \"soit\",\n",
    "                  \"soixante\", \"sommes\", \"son\", \"sont\", \"sous\", \"souvent\", \"soyez\", \"soyons\", \"specifique\",\n",
    "                  \"specifiques\", \"speculatif\", \"stop\", \"strictement\", \"subtiles\", \"suffisant\", \"suffisante\", \"suffit\",\n",
    "                  \"suis\", \"suit\", \"suivant\", \"suivante\", \"suivantes\", \"suivants\", \"suivre\", \"sujet\", \"superpose\", \"sur\",\n",
    "                  \"surtout\", \"t\", \"ta\", \"tac\", \"tandis\", \"tant\", \"tardive\", \"te\", \"tel\", \"telle\", \"tellement\", \"telles\",\n",
    "                  \"tels\", \"tenant\", \"tend\", \"tenir\", \"tente\", \"tes\", \"tic\", \"tien\", \"tienne\", \"tiennes\", \"tiens\", \"toc\",\n",
    "                  \"toi\", \"toi-même\", \"ton\", \"touchant\", \"toujours\", \"tous\", \"tout\", \"toute\", \"toutefois\", \"toutes\",\n",
    "                  \"treize\", \"trente\", \"tres\", \"trois\", \"troisième\", \"troisièmement\", \"trop\", \"très\", \"tsoin\", \"tsouin\",\n",
    "                  \"tu\", \"té\", \"u\", \"un\", \"une\", \"unes\", \"uniformement\", \"unique\", \"uniques\", \"uns\", \"v\", \"va\", \"vais\",\n",
    "                  \"valeur\", \"vas\", \"vers\", \"via\", \"vif\", \"vifs\", \"vingt\", \"vivat\", \"vive\", \"vives\", \"vlan\", \"voici\",\n",
    "                  \"voie\", \"voient\", \"voilà\", \"vont\", \"vos\", \"votre\", \"vous\", \"vous-mêmes\", \"vu\", \"vé\", \"vôtre\",\n",
    "                  \"vôtres\", \"w\", \"x\", \"y\", \"z\", \"zut\", \"à\", \"â\", \"ça\", \"ès\", \"étaient\", \"étais\", \"était\", \"étant\",\n",
    "                  \"état\", \"étiez\", \"étions\", \"été\", \"étée\", \"étées\", \"étés\", \"êtes\", \"être\", \"ô\",'.','...','?','!',':']\n",
    "    det = [\"le\", \"la\", \"les\", \"l'\", \"un\", \"une\", \"des\", \"d'\", \"du\", \"de\", \"au\", \"aux\", \"ce\", \"cet\", \"cette\", \"ces\",\n",
    "           \"mon\", \"son\", \"ma\", \"ta\", \"sa\", \"mes\", \"ses\", \"notre\", \"votre\", \"leur\", \"nos\", \"vos\", \"leurs\", \"aucun\",\n",
    "           \"aucune\", \"aucuns\", \"tel\", \"telle\", \"tels\", \"telles\", \"tout\", \"toute\", \"tous\", \"toutes\", \"chaque\"]\n",
    "    pron = [\"je\", \"tu\", \"il\", \"elle\", \"on\", \"nous\", \"vous\", \"ils\", \"elles\", \"me\", \"m'\", \"moi\", \"te\", \"t'\", \"toi\",\n",
    "              \"se\", \"y\", \"le\", \"lui\", \"soi\", \"leur\", \"eux\", \"lui\", \"qui\", \"que\", \"quoi\", \"dont\", \"où\"]\n",
    "    conj = [\"mais\", \"ou\", \"et\", \"donc\", \"or\", \"ni\", \"car\", \"que\", \"quand\", \"comme\", \"si\", \"lorsque\", \"quoique\",\n",
    "                   \"puisque\"]\n",
    "    prep = [\"à\", \"derrière\", \"malgré\", \"sauf\", \"selon\", \"avant\", \"devant\", \"sous\", \"avec\", \"en\", \"par\", \"sur\",\n",
    "                    \"entre\", \"parmi\", \"envers\", \"pendant\", \"vers\", \"dans\", \"pour\", \"de\", \"près\", \"depuis\", \"sans\"]\n",
    "\n",
    "    new_articles = {}\n",
    "    for i in tqdm_notebook(articles):\n",
    "        \n",
    "        new_articles[i] = articles[i]\n",
    "        \n",
    "        # Clean the text, get only relevant words\n",
    "        new_articles[i]['vocab'] = [word.lower() for word in textdata.words(new_articles[i]['content']+ new_articles[i]['content']) if len(word) > 2]\n",
    "        new_articles[i]['vocab'] = [word for word in new_articles[i]['vocab'] if word not in stop_words and word not in det and word not in pron and word not in conj and word not in prep]\n",
    "        new_articles[i]['vocab'] = [word for word in new_articles[i]['vocab'] if (\"http\" not in word) and (\"www\" not in word) and (\".\" not in word) and (\"(\" not in word)  and (\")\" not in word)  and (len(word) < 16)]\n",
    "        \n",
    "        # Get the tf for each word of the vocab for each article\n",
    "        new_articles[i]['words'] = tf(new_articles[i]['vocab'])\n",
    "        \n",
    "        # Get the pos tag for each word\n",
    "        pos_tagging = nltk.pos_tag(new_articles[i]['vocab'])\n",
    "        for pos_tag,word in zip(pos_tagging,new_articles[i]['words'].keys()):\n",
    "            title = 0\n",
    "            description = 0\n",
    "            if 'description' in  new_articles[i].keys() and word in new_articles[i]['description']:\n",
    "                description = 1\n",
    "            if word in new_articles[i]['title']:\n",
    "                title = 1\n",
    "            new_articles[i]['words'][word] = {'tf':new_articles[i]['words'][word],\n",
    "                                             'is_title':title,\n",
    "                                             'is_description':description,\n",
    "                                             'pos_tag':pos_tag[1]}\n",
    "            \n",
    "        if 'description' in  new_articles[i].keys():\n",
    "            del new_articles[i]['description']\n",
    "        del new_articles[i]['title']\n",
    "        del new_articles[i]['content']\n",
    "        del new_articles[i]['id_article']\n",
    "        del new_articles[i]['vocab']\n",
    "\n",
    "    return new_articles\n",
    "\n",
    "def tf(text):\n",
    "    \"\"\"\n",
    "        Summary:\n",
    "            this functions gives a dictionnaire where we have the TF associated\n",
    "            to each work in an article\n",
    "        In:\n",
    "            - text already tokenize\n",
    "        Out:\n",
    "            - TF for each word of the article gave in a dictionary\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(text, return_counts=True)\n",
    "    new_count = map(np.asscalar, counts)\n",
    "    dict_words = dict(zip(unique, new_count))\n",
    "    return dict_words\n",
    "\n",
    "\n",
    "def create_json_file(articles,name,path):\n",
    "    \"\"\"\n",
    "        Transform the list containing info on articles into json file\n",
    "        @param articles : all cleaned articles \n",
    "        @param i : the clean article id\n",
    "        @param path : path where the json files are stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(articles)\n",
    "        filename = 'clean_art_' + name+'.json'\n",
    "        with open(path + filename, 'w', encoding='utf-8') as file:\n",
    "            df.to_json(file,force_ascii=False) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal de l'environnnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scraped articles\n",
    "articles_jdle = json.loads(io.open('../data/articles/rough_art_jdle.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,description and title element for jdle articles\n",
    "for i in articles_jdle:\n",
    "    articles_jdle[str(i)]['content'] = tokenize(articles_jdle[str(i)]['content'])\n",
    "    articles_jdle[str(i)]['description'] = tokenize(articles_jdle[str(i)]['description'])\n",
    "    articles_jdle[str(i)]['title'] = tokenize(articles_jdle[str(i)]['title'])\n",
    "    \n",
    "# Cleaned articles\n",
    "clean_articles_jdle = clean_text(articles_jdle)\n",
    "create_json_file(clean_articles_jdle,'jdle','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scraped articles\n",
    "articles_lmde = json.loads(io.open('../data/articles/rough_art_lmde.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,description and title element for lmde articles\n",
    "for i in articles_lmde:\n",
    "    articles_lmde[str(i)]['content'] = clean_text(articles_lmde[str(i)]['content'])\n",
    "    articles_lmde[str(i)]['description'] = clean_text(articles_lmde[str(i)]['description'])\n",
    "    articles_lmde[str(i)]['title'] = clean_text(articles_lmde[str(i)]['title'])\n",
    "    \n",
    "# Cleaned articles\n",
    "clean_articles_lmde = get_token(articles_lmde)\n",
    "create_json_file(clean_articles_lmde,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7eme Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scraped articles\n",
    "articles_7econ = json.loads(io.open('../data/articles/rough_art_7econ.json', 'r', encoding='utf-8-sig').read())\n",
    "\n",
    "# Clean content,descirotion and title element for 7econ articles\n",
    "for i in articles_7econ:\n",
    "    articles_7econ[str(i)]['content'] = clean_text(articles_7econ[str(i)]['content'])\n",
    "    articles_7econ[str(i)]['title'] = clean_text(articles_7econ[str(i)]['title'])\n",
    "\n",
    "# Cleaned articles\n",
    "clean_articles_7econ = get_token(articles_7econ)\n",
    "create_json_file(clean_articles_7econ,'7econ','../data/articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
