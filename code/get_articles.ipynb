{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import collections as col\n",
    "import spacy \n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "import math\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions communes Ã  tous les types de journaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(url):\n",
    "    \"\"\"\n",
    "        The function parse the page with beautifulsoup\n",
    "        @param :  string containing the url of the rss feed\n",
    "        @return : object containing the parse page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "        data = req.text\n",
    "        soup = bs4.BeautifulSoup(data, \"lxml\")\n",
    "        return(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Remove the inapropriate caracters/words in order to clean the content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return nlp(text).text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def create_json_file(articles,name,path):\n",
    "    \"\"\"\n",
    "        Transform the list containing info on articles into  json file\n",
    "        @param articles : all articles scraped\n",
    "        @param i : the rough article id\n",
    "        @param path : path where the json files are stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(articles)\n",
    "        filename = 'rough_art_' + name+'.json'\n",
    "        with open(path + filename, 'w', encoding='utf-8') as file:\n",
    "            df.to_json(file, orient='index',force_ascii=False) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_url_articles_jdle(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    key_words = ['plastique','microplastique','dechet','ocean','pacifique']\n",
    "\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        \n",
    "        # Get all articles in a page\n",
    "        soup = parse_soup(website_link + \"/recherche=plastiques-\" + str(i) + \"?sort=pertinence\")\n",
    "\n",
    "        #Select only the link of the articles\n",
    "        for article_link,date in zip(soup.find_all(\"h2\", {\"class\":\"titreRecherche\"}),soup.find_all('span' ,{'class':\"color909090\"})):\n",
    "            article_link = article_link.find('a')\n",
    "            if article_link.get(\"href\") != None and '#' not in article_link.get(\"href\") and any(word in article_link.get(\"href\").lower() for word in key_words):\n",
    "                # link\n",
    "                new_article = website_link + article_link.get(\"href\")\n",
    "                \n",
    "                # date\n",
    "                date = date.text.lstrip()\n",
    "                year = parse(date, fuzzy=True).year\n",
    "                month = parse(date, fuzzy=True).month\n",
    "                if month <= 9:\n",
    "                    month = '0' + str(month)\n",
    "                day = parse(date, fuzzy=True).day\n",
    "                if day <= 9:\n",
    "                    day = '0' + str(day)\n",
    "                \n",
    "                articles.append([new_article,str(year)+'/'+str(month)+'/'+str(day)])\n",
    "                \n",
    "    return articles\n",
    "\n",
    "def extract_info_articles_jdle(articles_info):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return : dictionnary with info on articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,info in tqdm_notebook(enumerate(articles_info)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "\n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(info[0])\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('strong') != None:\n",
    "            description= soup.find('strong').text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('h1', {'class':\"articleTitre\"}) != None:\n",
    "                title=soup.find('h1', {'class':\"articleTitre\"}).text\n",
    "\n",
    "        # Get the authors and publication_date\n",
    "        posssible_authors = [soup.find('div',{\"class\":\"articleHautPageAuthor\"}),soup.find(\"span\",{\"class\":\"regular11px\"})]\n",
    "        for author in posssible_authors:\n",
    "            if author != None and 'par' in author and len(author.text.replace(' ','').split('par')[0]) <20:\n",
    "                authors = author.text.split('par')[1] # get only the author\n",
    "\n",
    "        # Get publication date       \n",
    "        pub_date = info[1] \n",
    "        \n",
    "        # Get contents : first method   \n",
    "        for bal in soup.find_all(\"p\", {\"class\":\"MsoNormal\"}):\n",
    "            for cont in bal.find_all('span'):\n",
    "                contents.append(cont.text)\n",
    "                \n",
    "        # Get contents : second method      \n",
    "        if not contents:\n",
    "            for cont in soup.find_all(\"p\"):\n",
    "                if cont != None and cont.find('strong') == None and len(cont.text) > 80:\n",
    "                    contents.append(cont.text)\n",
    "            contents = contents[:-3]\n",
    "            \n",
    "        # We only analyse article with a content\n",
    "        if contents:\n",
    "            # Clean the string variable\n",
    "            contents = clean_text(' '.join(contents))\n",
    "            title = clean_text(title)\n",
    "            description = clean_text(description)\n",
    "            authors = clean_text(authors)\n",
    "\n",
    "            new_articles.append({\n",
    "                \"id_article\":index,\n",
    "                \"link\":info[0],\n",
    "                \"title\":title,\n",
    "                \"description\":description,\n",
    "                \"content\":contents,\n",
    "                \"authors\":authors,\n",
    "                \"newspaper\":'JournalEnvironnement',\n",
    "                \"publication_date\":pub_date\n",
    "            })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46397b2cb0f84332affe90cc5f7b16f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link= \"http://www.journaldelenvironnement.net\"\n",
    "max_pages = 73 # specify the right number\n",
    "articles_jdle = get_url_articles_jdle(link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca8762561794f578d9f93da59e6fe1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_articles_jdle = extract_info_articles_jdle(articles_jdle)\n",
    "create_json_file(clean_articles_jdle,'jdle','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_lmde(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(website_link + '/recherche/?keywords=plastique+ocean&page_num=' +str(i) + '&operator=and&exclude_keywords=&qt=recherche_texte_titre&author=&period=since_1944&start_day=01&start_month=01&start_year=1944&end_day=07&end_month=02&end_year=2019&sort=pertinence')\n",
    "            \n",
    "            #Select only the link of the articles\n",
    "            for bal in soup.find_all('h3',{\"class\":\"txt4_120\"}):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None and 'article' in link:\n",
    "                    new_article = website_link + link\n",
    "                    link_articles.append(new_article)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_lmde(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('p',{\"class\":\"article__desc\"}) != None:\n",
    "            description= soup.find('p',{\"class\":\"article__desc\"}).text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('title') != None:\n",
    "                title = soup.find('title').text\n",
    "\n",
    "        # Get publication date\n",
    "        pub_date = link.split('article')[1][0:11]\n",
    "        year = parse(pub_date, fuzzy=True).year\n",
    "        month = parse(pub_date, fuzzy=True).month\n",
    "        if month <= 9:\n",
    "            month = '0' + str(month)\n",
    "        day = parse(pub_date, fuzzy=True).day\n",
    "        if day <= 9:\n",
    "            day = '0' + str(day)\n",
    "        pub_date = str(year) + '/'+ str(month)+'/' + str(day)\n",
    "\n",
    "        # Get the authors\n",
    "        if soup.find('span',{\"class\":\"meta__author\"}) != None:\n",
    "            authors = soup.find('span',{\"class\":\"meta__author\"}).text\n",
    "\n",
    "        # Get contents   \n",
    "        if soup.find(\"section\", {\"class\":\"article__content\"}) != None:\n",
    "            article_content = soup.find(\"section\", {\"class\":\"article__content\"})\n",
    "        for content in article_content.find_all('p'):\n",
    "            if content != None:\n",
    "                contents.append(content.text)    \n",
    "        \n",
    "        # Clean the string variable\n",
    "        contents = clean_text(' '.join(contents))\n",
    "        title = clean_text(title)\n",
    "        description = clean_text(description)\n",
    "        authors = clean_text(authors)\n",
    "        \n",
    "        new_articles.append({\n",
    "            \"id_article\":index,\n",
    "            \"link\":link,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"content\":contents,\n",
    "            \"authors\":authors,\n",
    "            \"newspaper\":'LeMonde',\n",
    "            \"publication_date\":pub_date\n",
    "        })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4846a74ccab24d10911b06e1dfc43339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=55), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.lemonde.fr\"\n",
    "max_pages = 56 # specify the right number\n",
    "articles_lmde = get_url_articles_lmde(link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877b595f71ad4321b036c18bcb8f3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_articles_lmde = get_info_articles_lmde(articles_lmde)\n",
    "create_json_file(clean_articles_lmde,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7econtinent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_7econ(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(website_link + 'page/'+str(i)+'/?s=plastique+ocean')\n",
    "            \n",
    "            #Select only the link of the articles\n",
    "            for bal in soup.find_all('div',{\"class\":\"search-entry-thumb\"}):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None:\n",
    "                    link_articles.append(link)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_7econ(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get publication date\n",
    "        if soup.find(\"time\") != None:\n",
    "            pub_date = str(soup.find(\"time\")).split(\"datetime=\")[1][1:11]\n",
    "            \n",
    "        # Get title\n",
    "        if soup.find(\"title\") != None:\n",
    "            title = soup.find(\"title\").text\n",
    "\n",
    "        # Get the authors\n",
    "        authors = \"7eme Continent\"\n",
    "\n",
    "        # Get contents   \n",
    "        for content in soup.find_all(\"p\"):\n",
    "            if content.find(\"strong\") == None:\n",
    "                contents.append(content.text)    \n",
    "        \n",
    "        if contents: # check if the article isn't a video or has a content\n",
    "            # Clean the string variable\n",
    "            contents = contents[:-10]\n",
    "            contents = clean_text(' '.join(contents))\n",
    "            title = clean_text(title)\n",
    "            authors = clean_text(authors)\n",
    "\n",
    "            new_articles.append({\n",
    "                \"id_article\":index,\n",
    "                \"link\":link,\n",
    "                \"title\":title,\n",
    "                \"content\":contents,\n",
    "                \"authors\":authors,\n",
    "                \"newspaper\":'7emeContinent',\n",
    "                \"publication_date\":pub_date\n",
    "            })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73dbf7f3c74c42a18a1518ec013c7539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link = \"http://www.septiemecontinent.com/\"\n",
    "max_pages = 17 # specify the right number (16)\n",
    "articles_7econ = get_url_articles_7econ(link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c583bc510b804af18533b32a53551e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_articles_7econ = get_info_articles_7econ(articles_7econ)\n",
    "clean_articles_7econ\n",
    "create_json_file(clean_articles_7econ,'7econ','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv files to keep the usefull links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_creation = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Jounral de l'environnement\n",
    "pd.DataFrame(articles_jdle).to_csv('../data/links/link_jdle_'+str(date_creation)+'.csv',sep=';',encoding='utf-8')\n",
    "\n",
    "# Le Monde\n",
    "pd.DataFrame(articles_lmde).to_csv('../data/links/link_ldme_'+str(date_creation)+'.csv',sep=';')\n",
    "\n",
    "# 7eme continent\n",
    "pd.DataFrame(articles_7econ).to_csv('../data/links/link_7econ_'+str(date_creation)+'.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
