{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import collections as col\n",
    "import spacy \n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "import math\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions communes à tous les types de journaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(url):\n",
    "    \"\"\"\n",
    "        The function parse the page with beautifulsoup\n",
    "        @param :  string containing the url of the rss feed\n",
    "        @return : object containing the parse page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "        data = req.text\n",
    "        soup = bs4.BeautifulSoup(data, \"lxml\")\n",
    "        return(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Remove the inapropriate caracters/words in order to clean the content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return nlp(text).text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def create_json_file(articles,name,path):\n",
    "    \"\"\"\n",
    "        Transform the list containing info on articles into  json file\n",
    "        @param articles : all articles scraped\n",
    "        @param i : the rough article id\n",
    "        @param path : path where the json files are stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_creation = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        df = pd.DataFrame(articles)\n",
    "        filename = 'art_'+date_creation+'_' + name+'.json'\n",
    "        with open(path + filename, 'w', encoding='utf-8') as file:\n",
    "            df.to_json(file, orient='index',force_ascii=False) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_url_articles_jdle(soup_link_website, website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    try:\n",
    "        link_articles = []\n",
    "        key_words = ['plastique','microplastique']\n",
    "        \n",
    "        # Get all pages of the website\n",
    "        for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(soup_link_website + '/' + str(i))\n",
    "\n",
    "            #Select only the link of the articles\n",
    "            for article_link in soup.find_all('a'):\n",
    "                if article_link.get(\"href\") != None and '#' not in article_link.get(\"href\") and any(word in article_link.get(\"href\").lower() for word in key_words):\n",
    "                    new_article = website_link + article_link.get(\"href\")\n",
    "                    link_articles.append(new_article)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return list(set(link_articles))\n",
    "\n",
    "def get_info_articles_jdle(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return : dictionnary with info on articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "\n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('strong') != None:\n",
    "            description= soup.find('strong').text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('h1', {'class':\"articleTitre\"}) != None:\n",
    "                title=soup.find('h1', {'class':\"articleTitre\"}).text\n",
    "\n",
    "        # Get the authors and publication_date\n",
    "        posssible_authors = [soup.find('div',{\"class\":\"articleHautPageAuthor\"}),soup.find(\"span\",{\"class\":\"regular11px\"})]\n",
    "        for author in posssible_authors:\n",
    "            if author != None and 'par' in author and len(author.text.replace(' ','').split('par')[0]) <20:\n",
    "                authors = author.text.split('par')[1] # get only the author\n",
    "                pub_date = author.text.replace(' ','').split('par')[0][2:] # get only the date\n",
    "\n",
    "        # Get the date at the right format\n",
    "#         dictio_month = {'janvier':'01','février':'02','mars':'03','avril':'04','mai':'05','juin':'06','juillet':'07','août':'08','septembre':'09','octobre':'10','novembre':'11','décembre':'12'}\n",
    "#         year = pub_date[-4:]\n",
    "#         month = '01'\n",
    "#         for i,el in enumerate(dictio_month):\n",
    "#             if el in pub_date:\n",
    "#                 if i < 9:\n",
    "#                     month = '0'+str(i+1)\n",
    "#                 else:\n",
    "#                     month = str(i+1)\n",
    "#         day = pub_date[0:2]\n",
    "#         pub_date = str(year+'/'+month+'/'+day)\n",
    "\n",
    "        # Get contents   \n",
    "        for bal in soup.find_all(\"p\", {\"class\":\"MsoNormal\"}):\n",
    "            for cont in bal.find_all('span'):\n",
    "                contents.append(cont.text) # regardez les contenus par toujours là ou juste dégeu\n",
    "            \n",
    "        # Clean the string variable\n",
    "        contents = clean_text(' '.join(contents))\n",
    "        title = clean_text(title)\n",
    "        description = clean_text(description)\n",
    "        authors = clean_text(authors)\n",
    "        \n",
    "        new_articles.append({\n",
    "            \"id_article\":index,\n",
    "            \"link\":link,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"content\":contents,\n",
    "            \"authors\":authors,\n",
    "            \"publication_date\":pub_date\n",
    "        })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> On se concentre sur les thèmes eau, déchets et climat proposés par le journal de l'environnement \n",
    "dans le but de récupérer les liens qui nous sont utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = [\"eau\",\"dechets\",\"climat\"]\n",
    "nb_pages = [370,260,180]\n",
    "link= \"http://www.journaldelenvironnement.net\"\n",
    "path = '../data/links/url_articles_jdle.csv'\n",
    "\n",
    "link_articles_jdle = []\n",
    "for i,theme in tqdm_notebook(enumerate(themes)):\n",
    "    link_articles_jdle.append(get_url_articles_jdle(link+'/'+themes[i],link,nb_pages[i]))\n",
    "    \n",
    "link_articles_jdle = list(set(link_articles_jdle[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_articles_jdle = get_info_articles_jdle(link_articles_jdle)\n",
    "create_json_file(clean_articles_jdle,'jlde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_lmde(soup_link_website, article_link, start, end):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    key_words = ['plastique','microplastique']\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(start,end)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(soup_link_website + str(i) + '.html')\n",
    "            \n",
    "            #Select only the link of the articles\n",
    "            for bal in soup.find_all('h3',{\"class\":\" \"}):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None and 'article' in link and any(word in link for word in key_words):\n",
    "                    new_article = article_link + link\n",
    "                    link_articles.append(new_article)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_lmde(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('p',{\"class\":\"article__desc\"}) != None:\n",
    "            description= soup.find('p',{\"class\":\"article__desc\"}).text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('h1',{\"class\":\"article_title\"}) != None:\n",
    "                title = soup.find('h1',{\"class\":\"article_title\"}).text\n",
    "\n",
    "        # Get publication date\n",
    "        pub_date = link[40:50]\n",
    "\n",
    "        # Get the authors\n",
    "        if soup.find('span',{\"class\":\"meta__author\"}) != None:\n",
    "            authors = soup.find('span',{\"class\":\"meta__author\"}).text\n",
    "\n",
    "        # Get contents   \n",
    "        if soup.find(\"section\", {\"class\":\"article__content\"}) != None:\n",
    "            article_content = soup.find(\"section\", {\"class\":\"article__content\"})\n",
    "        for content in article_content.find_all('p'):\n",
    "            if content != None:\n",
    "                contents.append(content.text)    \n",
    "        \n",
    "        # Clean the string variable\n",
    "        contents = clean_text(' '.join(contents))\n",
    "        title = clean_text(title)\n",
    "        description = clean_text(description)\n",
    "        authors = clean_text(authors)\n",
    "        \n",
    "        new_articles.append({\n",
    "            \"id_article\":index,\n",
    "            \"link\":link,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"content\":contents,\n",
    "            \"authors\":authors,\n",
    "            \"publication_date\":pub_date\n",
    "        })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> On dans un premier temps avec n pages (selectionnées) sur la catégorie planète du journal le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.lemonde.fr/\"\n",
    "start_page = 1\n",
    "end_page=100 # choisir un nombre qui fonctionne\n",
    "\n",
    "link_articles_lmde = get_url_articles_lmde(link+'planete/',link,start_page,end_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> On récupère les liens jugées utiles, puis on place le contenu de chaque article dans un fichier json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_articles_lmde = get_info_articles_lmde(link_articles_lmde)\n",
    "create_json_file(clean_articles_lmde,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'Humanité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_hum(soup_link_website, article_link, start, end):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    key_words = ['plastique','microplastique']\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(start,end)):\n",
    "        # Get all articles in a page\n",
    "        soup = parse_soup(soup_link_website + '?page='+str(i))\n",
    "\n",
    "        #Select only the link of the articles\n",
    "        for bal in soup.find_all('div',{\"class\":\"field-item even\"}):\n",
    "            if bal.find('a') != None:\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None and 'humanite' in link and any(word in link for word in key_words):\n",
    "                    link_articles.append(link)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_hum(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('p',{\"class\":\"article__desc\"}) != None:\n",
    "            description= soup.find('p',{\"class\":\"article__desc\"}).text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('h1',{\"class\":\"article_title\"}) != None:\n",
    "                title = soup.find('h1',{\"class\":\"article_title\"}).text\n",
    "\n",
    "        # Get publication date\n",
    "        pub_date = link[40:50]\n",
    "\n",
    "        # Get the authors\n",
    "        if soup.find('span',{\"class\":\"meta__author\"}) != None:\n",
    "            authors = soup.find('span',{\"class\":\"meta__author\"}).text\n",
    "\n",
    "        # Get contents   \n",
    "        if soup.find(\"section\", {\"class\":\"article__content\"}) != None:\n",
    "            article_content = soup.find(\"section\", {\"class\":\"article__content\"})\n",
    "        for content in article_content.find_all('p'):\n",
    "            if content != None:\n",
    "                contents.append(content.text)    \n",
    "        \n",
    "        # Clean the string variable\n",
    "        contents = clean_text(' '.join(contents))\n",
    "        title = clean_text(title)\n",
    "        description = clean_text(description)\n",
    "        authors = clean_text(authors)\n",
    "        \n",
    "        new_articles.append({\n",
    "            \"id_article\":index,\n",
    "            \"link\":link,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"content\":contents,\n",
    "            \"authors\":authors,\n",
    "            \"publication_date\":pub_date\n",
    "        })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af852a1d93444aafa6fad1408ce726ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.humanite.fr/le-parlement-europeen-vote-linterdiction-des-plastiques-jetables-662807',\n",
       " 'https://www.humanite.fr/la-france-mauvaise-eleve-en-matiere-de-recyclage-du-plastique-632777',\n",
       " 'https://www.humanite.fr/nathalie-gontard-lavenir-ce-nest-en-aucun-cas-le-plastique-656299',\n",
       " 'https://www.humanite.fr/environnement-le-sac-en-plastique-se-fait-soluble-659731']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = \"https://www.humanite.fr/\"\n",
    "start_page = 1\n",
    "end_page=296\n",
    "\n",
    "link_articles_hum = get_url_articles_hum(link+'environnement',link,start_page,end_page)\n",
    "link_articles_hum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv files to keep the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jounral de l'environnement\n",
    "pd.DataFrame(link_articles_jdle).to_csv('../data/links/',sep=';')\n",
    "\n",
    "# Le Monde\n",
    "pd.DataFrame(link_articles_lmde).to_csv('../data/links/',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
