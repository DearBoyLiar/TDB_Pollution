{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import collections as col\n",
    "import spacy \n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "import math\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions communes à tous les types de journaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_soup(url):\n",
    "    \"\"\"\n",
    "        The function parse the page with beautifulsoup\n",
    "        @param :  string containing the url of the rss feed\n",
    "        @return : object containing the parse page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "        data = req.text\n",
    "        soup = bs4.BeautifulSoup(data, \"lxml\")\n",
    "        return(soup)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Remove the inapropriate caracters/words in order to clean the content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return nlp(text).text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def create_json_file(articles,name,path):\n",
    "    \"\"\"\n",
    "        Transform the list containing info on articles into  json file\n",
    "        @param articles : all articles scraped\n",
    "        @param i : the rough article id\n",
    "        @param path : path where the json files are stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_creation = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        df = pd.DataFrame(articles)\n",
    "        filename = 'rough_art_'+date_creation+'_' + name+'.json'\n",
    "        with open(path + filename, 'w', encoding='utf-8') as file:\n",
    "            df.to_json(file, orient='index',force_ascii=False) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journal de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_url_articles_jdle(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    key_words = ['plastique','microplastique','dechet','ocean','pacifique']\n",
    "\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        \n",
    "        # Get all articles in a page\n",
    "        soup = parse_soup(website_link + \"/recherche=plastiques-\" + str(i) + \"?sort=pertinence\")\n",
    "\n",
    "        #Select only the link of the articles\n",
    "        for article_link,date in zip(soup.find_all(\"h2\", {\"class\":\"titreRecherche\"}),soup.find_all('span' ,{'class':\"color909090\"})):\n",
    "            article_link = article_link.find('a')\n",
    "            if article_link.get(\"href\") != None and '#' not in article_link.get(\"href\") and any(word in article_link.get(\"href\").lower() for word in key_words):\n",
    "                # link\n",
    "                new_article = website_link + article_link.get(\"href\")\n",
    "                \n",
    "                # date\n",
    "                date = date.text.lstrip()\n",
    "                year = parse(date, fuzzy=True).year\n",
    "                month = parse(date, fuzzy=True).month\n",
    "                if month <= 9:\n",
    "                    month = '0' + str(month)\n",
    "                day = parse(date, fuzzy=True).day\n",
    "                if day <= 9:\n",
    "                    day = '0' + str(day)\n",
    "                \n",
    "                articles.append([new_article,str(year)+'/'+str(month)+'/'+str(day)])\n",
    "                \n",
    "    return articles\n",
    "\n",
    "def extract_info_articles_jdle(articles_info):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return : dictionnary with info on articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,info in tqdm_notebook(enumerate(articles_info)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "\n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(info[0])\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('strong') != None:\n",
    "            description= soup.find('strong').text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('h1', {'class':\"articleTitre\"}) != None:\n",
    "                title=soup.find('h1', {'class':\"articleTitre\"}).text\n",
    "\n",
    "        # Get the authors and publication_date\n",
    "        posssible_authors = [soup.find('div',{\"class\":\"articleHautPageAuthor\"}),soup.find(\"span\",{\"class\":\"regular11px\"})]\n",
    "        for author in posssible_authors:\n",
    "            if author != None and 'par' in author and len(author.text.replace(' ','').split('par')[0]) <20:\n",
    "                authors = author.text.split('par')[1] # get only the author\n",
    "\n",
    "        # Get publication date       \n",
    "        pub_date = info[1] \n",
    "        \n",
    "        # Get contents : first method   \n",
    "        for bal in soup.find_all(\"p\", {\"class\":\"MsoNormal\"}):\n",
    "            for cont in bal.find_all('span'):\n",
    "                contents.append(cont.text)\n",
    "                \n",
    "        # Get contents : second method      \n",
    "        if not contents:\n",
    "            # des p sans storng et vec longueur supérieure à 200\n",
    "            for cont in soup.find_all(\"p\"):\n",
    "                if cont != None and cont.find('strong') == None and len(cont.text) > 80:\n",
    "                    contents.append(cont.text)\n",
    "            contents = contents[:-3]\n",
    "            \n",
    "        # We only analyse article with a content\n",
    "        if contents:\n",
    "            # Clean the string variable\n",
    "            contents = clean_text(' '.join(contents))\n",
    "            title = clean_text(title)\n",
    "            description = clean_text(description)\n",
    "            authors = clean_text(authors)\n",
    "\n",
    "            new_articles.append({\n",
    "                \"id_article\":index,\n",
    "                \"link\":info[0],\n",
    "                \"title\":title,\n",
    "                \"description\":description,\n",
    "                \"content\":contents,\n",
    "                \"authors\":authors,\n",
    "                \"publication_date\":pub_date\n",
    "            })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2379efb4b1f4a5d8deb2d17ed60596e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=72), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "link= \"http://www.journaldelenvironnement.net\"\n",
    "max_pages = 73 # specify the right number\n",
    "articles_jdle = get_url_articles_jdle(link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7ba1e2fe1f4176ba14454cec0c79b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_articles_jdle = extract_info_articles_jdle(articles_jdle)\n",
    "create_json_file(clean_articles_jdle,'jlde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le Monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_lmde(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(website_link + '/recherche/?keywords=plastiques&page_num=' +str(i) + '&operator=and&exclude_keywords=&qt=recherche_texte_titre&author=&period=since_1944&start_day=01&start_month=01&start_year=1944&end_day=07&end_month=02&end_year=2019&sort=pertinence')\n",
    "            \n",
    "            #Select only the link of the articles\n",
    "            for bal in soup.find_all('h3',{\"class\":\"txt4_120\"}):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None and 'article' in link:\n",
    "                    new_article = website_link + link\n",
    "                    link_articles.append(new_article)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_lmde(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        if soup.find('p',{\"class\":\"article__desc\"}) != None:\n",
    "            description= soup.find('p',{\"class\":\"article__desc\"}).text\n",
    "\n",
    "        # Get title\n",
    "        if soup.find('title') != None:\n",
    "                title = soup.find('title').text\n",
    "\n",
    "        # Get publication date\n",
    "        pub_date = link.split('article')[1][0:11]\n",
    "        year = parse(pub_date, fuzzy=True).year\n",
    "        month = parse(pub_date, fuzzy=True).month\n",
    "        if month <= 9:\n",
    "            month = '0' + str(month)\n",
    "        day = parse(pub_date, fuzzy=True).day\n",
    "        if day <= 9:\n",
    "            day = '0' + str(day)\n",
    "        pub_date = str(year) + '/'+ str(month)+'/' + str(day)\n",
    "\n",
    "        # Get the authors\n",
    "        if soup.find('span',{\"class\":\"meta__author\"}) != None:\n",
    "            authors = soup.find('span',{\"class\":\"meta__author\"}).text\n",
    "\n",
    "        # Get contents   \n",
    "        if soup.find(\"section\", {\"class\":\"article__content\"}) != None:\n",
    "            article_content = soup.find(\"section\", {\"class\":\"article__content\"})\n",
    "        for content in article_content.find_all('p'):\n",
    "            if content != None:\n",
    "                contents.append(content.text)    \n",
    "        \n",
    "        # Clean the string variable\n",
    "        contents = clean_text(' '.join(contents))\n",
    "        title = clean_text(title)\n",
    "        description = clean_text(description)\n",
    "        authors = clean_text(authors)\n",
    "        \n",
    "        new_articles.append({\n",
    "            \"id_article\":index,\n",
    "            \"link\":link,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"content\":contents,\n",
    "            \"authors\":authors,\n",
    "            \"publication_date\":pub_date\n",
    "        })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c325d0e5124fcc8fb15f1a0efd1ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=54), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "link = \"https://www.lemonde.fr\"\n",
    "max_pages = 55 # specify the right number\n",
    "articles_lmde = get_url_articles_lmde(link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd0f3a3872e46f38ad573092d7fc3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_articles_lmde = get_info_articles_lmde(articles_lmde)\n",
    "create_json_file(clean_articles_lmde,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les Echos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_lecho(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    key_words = ['plastique','microplastique','dechet','pollution']\n",
    "    base = 'https://www.lesechos.fr'\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(website_link+str(i))\n",
    "            \n",
    "            #Select only the link of the articles and dates\n",
    "            for bal,date in zip(soup.find_all(\"h2\" ,{\"class\":\"style-titre\"}),soup.find_all(\"time\",{\"class\":\"meta-date\"})):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None and base in link and any(word in link for word in key_words):\n",
    "                    # date\n",
    "                    date = date.text.lstrip()\n",
    "                    year = parse(date, fuzzy=True).year\n",
    "                    month = parse(date, fuzzy=True).month\n",
    "                    if month <= 9:\n",
    "                        month = '0' + str(month)\n",
    "                    day = parse(date, fuzzy=True).day\n",
    "                    if day <= 9:\n",
    "                        day = '0' + str(day)\n",
    "                    \n",
    "                    link_articles.append([link,str(year)+'/'+str(month)+'/'+str(day)])\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return link_articles\n",
    "\n",
    "def get_info_articles_lecho(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link[0])\n",
    "\n",
    "        # Get title\n",
    "        if soup.find(\"h1\",{\"class\":\"title-article\"}) != None:\n",
    "            title = soup.find(\"h1\" ,{\"class\":\"title-article\"}).text\n",
    "\n",
    "        # Get publication date\n",
    "        pub_date = link[1]\n",
    "\n",
    "        # Get the authors\n",
    "        if soup.find(\"a\" ,{\"class\":\"meta-author\"}) != None:\n",
    "            base_authors =  soup.find(\"a\" ,{\"class\":\"meta-author\"}).find('span')\n",
    "            if base_authors != None:\n",
    "                authors = base_authors.text\n",
    "\n",
    "        # Get contents   \n",
    "        for content in soup.find_all(\"p\", {'itemprop':\"articleBody\"}):\n",
    "            contents.append(content.text)    \n",
    "        \n",
    "        if contents: # check if the article isn't a video or has a content\n",
    "            # Clean the string variable\n",
    "            contents = clean_text(' '.join(contents))\n",
    "            title = clean_text(title)\n",
    "            authors = clean_text(authors)\n",
    "\n",
    "            new_articles.append({\n",
    "                \"id_article\":index,\n",
    "                \"link\":link[0],\n",
    "                \"title\":title,\n",
    "                \"content\":contents,\n",
    "                \"authors\":authors,\n",
    "                \"publication_date\":pub_date\n",
    "            })\n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08728a7f8494f76807a23df0a4e0894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_link = 'http://recherche.lesechos.fr/recherche.php?exec=1&texte=plastique+ocean&ob=globalrelevance&page='\n",
    "max_pages = 8 # specify the right number\n",
    "articles_lecho = get_url_articles_lecho(search_link,max_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80312ceedb5c44afb56a556880ae7b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_articles_lecho = get_info_articles_lecho(articles_lecho)\n",
    "create_json_file(clean_articles_lecho,'lecho','../data/articles/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7econtinent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_articles_7econ(website_link, nb_max_pages):\n",
    "    \"\"\"\n",
    "        Based on the links passed in parameter, we get the url articles of the website\n",
    "        @return :  A list of links of articles\n",
    "    \"\"\"\n",
    "    link_articles = []\n",
    "    # Get all pages of the website\n",
    "    for i in tqdm_notebook(range(1,nb_max_pages)):\n",
    "        try:\n",
    "            # Get all articles in a page\n",
    "            soup = parse_soup(website_link + '/page/'+str(i)+'/?s=plastique+ocean')\n",
    "            \n",
    "            #Select only the link of the articles\n",
    "            for bal in soup.find_all('div',{\"class\":\"search-entry-thumb\"}):\n",
    "                link = bal.find('a').get('href')\n",
    "                if link != None:\n",
    "                    link_articles.append(link)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return list(set(link_articles))\n",
    "\n",
    "\n",
    "def get_info_articles_7econ(link_articles):\n",
    "    \"\"\"\n",
    "        We get the content of the different articles\n",
    "        @return ! dictionnary with the sentences by articles\n",
    "    \"\"\"\n",
    "    new_articles = []\n",
    "    for index,link in tqdm_notebook(enumerate(link_articles)):\n",
    "        # Initilialize variables\n",
    "        contents = []\n",
    "        description = title = pub_date = authors = \"\"\n",
    "        \n",
    "        # Get the content of the page relative to an article\n",
    "        soup = parse_soup(link)\n",
    "\n",
    "        # Get the description\n",
    "        for text in soup.find_all('p'):\n",
    "            contents.append(text)\n",
    "\n",
    "                \n",
    "        if soup.find(\"time\") != None:\n",
    "            pub_date = str(soup.find(\"time\")).split(\"datetime=\")[1][1:11]\n",
    "            \n",
    "    return new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc404f846784e8b9103367e927a1c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://www.septiemecontinent.com/scientifiques-de-lexpedition-7e-continent-prouvent-presence-de-nanoparticules-de-plastique-locean/',\n",
       " 'http://www.septiemecontinent.com/nouvelles-publications-scientifiques-pollution-plastique-locean/',\n",
       " 'http://www.septiemecontinent.com/proteger-locean-ca-sapprend/',\n",
       " 'http://www.septiemecontinent.com/green-cross-les-expeditions-7contient-sassocient-lutte-contre-pollution-oceans-les-dechets-plastique/',\n",
       " 'http://www.septiemecontinent.com/250-millions-de-tonnes-de-dechets-en-plastique-accumules-dans-nos-oceans-dici-2025/',\n",
       " 'http://www.septiemecontinent.com/pollution-plastique-grandes-quantites-de-metaux-lourds-locean/',\n",
       " 'http://www.septiemecontinent.com/270-000-tonnes-plastique-flottent-les-oceans-du-monde/',\n",
       " 'http://www.septiemecontinent.com/journee-mondiale-oceans-letat-lieux-alarmant-de-lexpedition-7e-continent/',\n",
       " 'http://www.septiemecontinent.com/7e-continent-nouveau-voilier-porte-drapeau-de-lutte-contre-pollution-plastique-mers-oceans/',\n",
       " 'http://www.septiemecontinent.com/pollution-oceans-fragmentation-dechets-plastiques/']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = \"http://www.septiemecontinent.com\"\n",
    "max_pages = 2 # specify the right number (16)\n",
    "articles_7econ = get_url_articles_7econ(link,max_pages)\n",
    "articles_7econ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9506e5599c46f3aa8a9bc2bf4f8272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-04\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_articles_7econ = get_info_articles_7econ(['http://www.septiemecontinent.com/scientifiques-de-lexpedition-7e-continent-prouvent-presence-de-nanoparticules-de-plastique-locean/'])\n",
    "clean_articles_7econ\n",
    "#create_json_file(clean_articles_7econ,'lmde','../data/articles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "h2 class=title and a.get('href')\n",
    "journal in href\n",
    "date = _/_/date (160818)\n",
    "author = a class=journalist'.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create csv files to keep the usefull links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Jounral de l'environnement\n",
    "# pd.DataFrame(link_articles_jdle).to_csv('../data/links/',sep=';')\n",
    "\n",
    "# # Le Monde\n",
    "# pd.DataFrame(link_articles_lmde).to_csv('../data/links/',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
